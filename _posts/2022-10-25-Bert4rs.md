---
layout: post
title: Bert4Rec
tags: [微调, 自监督]
---
[BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer](
https://doi.org/10.48550/arXiv.1904.06690
)

# 方法概括
1. 使用双向自注意力网络对用户行为序列进行建模。据论文表述，这是第一个将深度**双向**序列模型和**完形填空**目标引入推荐系统领域的研究。
2. 预训练时目标任务完形填空，掩码
3. “完型填空”的训练方式与推荐任务不符（序列预测），在序列末尾增加一个标记[mask]，然后根据该标记的最终隐藏表示预测下一个项目
4. 为了更好地匹配顺序推荐任务（即预测最后一项），生成了在训练期间仅掩盖输入序列中最后一项的样本。它的作用类似于顺序推荐的微调，可以进一步提高推荐性能。



# 模型介绍
![](/PreRec/assets/fig/bert4rs.png)

# 实验
- 与8个baseline比较

- 证明了使用“完型填空”方法以及双向序列都对提高性能有关键作用。双向建模对于用户行为序列建模是必不可少的和有益的。

- 消融实验。位置嵌入，前馈网络，transformer层数，注意力头数等。

  


---
# 数据集

Beauty http://jmcauley.ucsd.edu/data/amazon/

Steam   https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data

MovieLens https://grouplens.org/datasets/movielens


