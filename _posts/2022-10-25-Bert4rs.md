---
layout: post
title:PeterRec
tags: [微调, 迁移学习]
---
[BParameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation](
https://doi.org/10.1145/3397271.3401156
)

# 方法概括
1. 开发一个高效通用的预训练模型。模型主要有基于rnn，cnn，注意力的，本文使用的是基于cnn的NextItNet。预训练目标：联合概率对依赖关系显示建模（单向的），完形填空式（类似于bert，双向），不同的目标体现在不同的残差项的目标函数中。
2. 微调架构：在用户序列末尾增加一个[TCL]，输入因果cnn，获得最终隐藏层向量，输入fnn网络获得最终结果。topn和分类任务目标函数分别是BPR,CE![image-20221025165143604](C:\Users\adminsiter\AppData\Roaming\Typora\typora-user-images\image-20221025165143604.png)
3. 让微调时，不同的下游任务共享大部分参数，也就是微调时冻结预训练时的参数，主要训练模型补丁的参数。
4. 模型补丁主要有串行，并行方式，串行插入补丁时候位置与数量比较灵活。并行插入时，归一化前的插入效果更好。
5. 证明了所提出的PeterRec通过对预训练模型进行微调，相对于从零训练，显著提高了下游推荐任务的准确性。



# 模型介绍
![image-20221025165446596](C:\Users\adminsiter\AppData\Roaming\Typora\typora-user-images\image-20221025165446596.png)

![image-20221025181401273](C:\Users\adminsiter\AppData\Roaming\Typora\typora-user-images\image-20221025181401273.png)

# 实验
- 问题一：预训练的效果。与没有预训练过的模型比较，效果好的多。

- 问题二：模型补丁的效果。只微调分类层，只微调最后一层cnn与分类层，全部微调。微调补丁的效果和全部微调效果相似，领先于只微调部分层。

- 问题三：可以预测什么样的用户配置，冷启动问题。与 数个基线对比（包括dupn），所有任务均更好，参数更少。

- 消融实验。数据有限时，补丁比fineALL效果更好，而且防止过拟合。

  


---
# 数据集

coldrec：https://github.com/fajieyuan/sigir2020_peterrec



 
