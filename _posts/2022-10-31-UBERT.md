---
layout: post
title: U-BERT
tags: [多辅任务, 自监督]
---
[U-BERT: Pre-training User Representations for Improved Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/16557)

# 方法概括
1. U-BERT目标是利用评论丰富domain的信息去提升评论缺乏domain的推荐性能（评分预测）。整体模型以BERT作为backbone。
2. 预训练：Review Encoder + User Encoder(embedding + work-level aggregation + fusion layer)。提出两个新任务(自监督任务 Masked Opinion Token Prediction(这里mask word是所有domain共有的opinion word) 和 评分预测任务 Opinion Rating Prediction)联合学习
3. 微调：Input layer(domain embedding + user embedding + users' reviews + item embedding + item's reviews) + Review Encoder + User & Item Encoder + Review Co-Matching Layer。任务是推荐评分预测。

# 模型介绍

### Motivation
![](/PreRec_CDR/assets/fig/10.png)

### Pre-train
![](/PreRec_CDR/assets/fig/11.png)

### Fine-Tuning
![](/PreRec_CDR/assets/fig/12.png)

# 实验部分
1. 将数据集（包括Yelp和Amazon Product）分为预训练数据集和微调数据集，需保证用户是相同共有的。
2. 消融实验：去掉Task1，去掉Task2，Task1随机mask
3. 改变预训练数据集大小，说明预训练数据越多，下游增益越好
4. 样例分析：可视化用户评论中word与物品评论中word的权重