---
layout: post
title: PeterRec
tags: [任务迁移, 自监督]
---
[Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation](
https://doi.org/10.1145/3397271.3401156
)

# 方法概括
1. 采用基于因果序列（NextItNey）或非因果序列（GRec）的CNN架构模型，随机mask（源数据集）序列中的物品作为自监督学习的预训练任务，学习通用的用户表征，目标是应用到多个目标数据集上的跨域推荐或者用户画像预测任务。
2. 预训练+微调框架。PeterRec是第一个在多域推荐场景中考虑存储高效的模型，代替微调整个模型或者添加最后若干层并重训的方式，本模型仅优化额外添加的串行或并行Model Patch（具体使用1x1的bottlenet结构）和预测层。


# 模型介绍
![](/PreRec/assets/fig/peterRec1.png)

![](/PreRec/assets/fig/peterRec2.png)


# 实验
- 精调预训练模型和随机预训练模型对比，验证了自监督预训练序列推荐任务显著提升下游任务表现。
- FairCLS（仅微调添加的预测层）< FairLast（微调最后的CNN层和预测层）< FairAll（微调所有参数）< PeterRec
- 与DUPN在用户画像预测和冷启动任务下，发现PeterRec效果更好，因为多任务预训练得到的参数不能保证其也是微调阶段的最优。
- 不同模型之间，预训练效果更好并不意味着下游任务也更好；相同模型之间，预训练效果更好通常意味着下游任务表现也会更好。
- 与FairAll相比，当下游任务数据稀疏时，Peter表现更加稳定，不容易过拟合。

  


---
# 数据集

coldrec：https://github.com/fajieyuan/sigir2020_peterrec



 
